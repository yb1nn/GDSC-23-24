{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWHTNMii5Og3sH+DMuOvZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yb1nn/GDSC-23-24/blob/Headington/Mamma_Map_AI_Data_Team_Final_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD9A4NcdQ-B2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "# 1. 사용자 ID, 위도, 경도를 하드코딩된 값으로 가정\n",
        "user_id = \"user123\"\n",
        "user_location = np.array([[35.116315938499866, 128.967294753387]])  # 가상의 사용자 위치\n",
        "\n",
        "# 2. kNN 알고리즘을 사용하여 수유실을 추려냅니다.\n",
        "\n",
        "# CSV 파일 읽기 (한국어 인코딩 처리)\n",
        "file_path = 'C:\\\\Users\\\\win\\\\Documents\\\\GitHub\\\\MamMaMap\\\\Data\\\\myfile.csv'\n",
        "data = pd.read_csv(file_path, encoding='cp949')\n",
        "\n",
        "# 수유실의 경도와 위도 데이터 추출\n",
        "nursing_rooms = data[['경도', '위도']]\n",
        "\n",
        "# kNN 모델을 사용하여 가장 가까운 20개의 수유실을 찾습니다.\n",
        "knn = NearestNeighbors(n_neighbors=20)\n",
        "knn.fit(nursing_rooms)\n",
        "\n",
        "# 사용자 위치에 가장 가까운 20개의 수유실을 찾습니다.\n",
        "distances, indices = knn.kneighbors(user_location)\n",
        "\n",
        "print(distances, indices)\n",
        "# 3. 수유실의 [리뷰, 평균 평점] 데이터를 불러옴\n",
        "review_file_path = 'C:\\\\Users\\\\win\\\\Documents\\\\GitHub\\\\MamMaMap\\\\Data\\\\biased_nursing_room_reviews_with_facility_order(리뷰,평점데이터).csv'\n",
        "try:\n",
        "    review_data = pd.read_csv(review_file_path, encoding='utf-8')\n",
        "except UnicodeDecodeError:\n",
        "    review_data = pd.read_csv(review_file_path, encoding='utf-16')\n",
        "\n",
        "    # 수유실별로 리뷰 텍스트를 리스트로 묶고 평균 평점 계산\n",
        "reviews_ratings = review_data.groupby('facility').agg({'review_text': list, 'rating': 'mean'})\n",
        "\n",
        "    # 인덱스 재설정\n",
        "reviews_ratings.reset_index(inplace=True)\n",
        "reviews_ratings = reviews_ratings['rating'].tolist()\n",
        "# 결과 출력\n",
        "print(\"reviews_ratings\")\n",
        "print(reviews_ratings, sep = '\\n')\n",
        "#============================================================================================\n",
        "#=========================================== 감정 점수========================================\n",
        "#============================================================================================\n",
        "# 4. 각 리뷰에 대해 감성 분석을 시뮬레이션하여 감정 점수를 도출\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "df = pd.read_csv(review_file_path, encoding='utf-8')\n",
        "# 정규 표현식 함수 정의\n",
        "\n",
        "import re\n",
        "\n",
        "def apply_regular_expression(text):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 한글 추출 규칙: 띄어 쓰기(1 개)를 포함한 한글\n",
        "    result = hangul.sub('', text)  # 위에 설정한 \"hangul\"규칙을 \"text\"에 적용(.sub)시킴\n",
        "    return result\n",
        "#특수문자 제거\n",
        "apply_regular_expression(df['review_text'][0])\n",
        "from konlpy.tag import Okt\n",
        "from collections import Counter\n",
        "okt = Okt()  # 명사 형태소 추출 함수\n",
        "nouns = okt.nouns(apply_regular_expression(df['review_text'][0]))\n",
        "nouns\n",
        "# 말뭉치 생성\n",
        "corpus = \"\".join(df['review_text'].tolist())\n",
        "corpus\n",
        "# 정규 표현식 적용\n",
        "apply_regular_expression(corpus)\n",
        "# 전체 말뭉치(corpus)에서 명사 형태소 추출\n",
        "nouns = okt.nouns(apply_regular_expression(corpus))\n",
        "print(nouns)\n",
        "counter = Counter(nouns)\n",
        "counter.most_common(10)\n",
        "#한글자 명사 제거\n",
        "available_counter = Counter({x: counter[x] for x in counter if len(x) > 1})\n",
        "available_counter.most_common(10)\n",
        "#한국어 불용어 사전\n",
        "stopwords = pd.read_csv(\"https://raw.githubusercontent.com/yoonkt200/FastCampusDataset/master/korean_stopwords.txt\").values.tolist()\n",
        "stopwords[:10]\n",
        "#데이터셋에 특화된 불용어 처리 ex)수유실\n",
        "nursing_room_stopwords = ['수유실', '부산']\n",
        "for word in nursing_room_stopwords:\n",
        "    stopwords.append(word)\n",
        "#워드 카운트\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def text_cleaning(text):\n",
        "    hangul = re.compile('[^ ㄱ-ㅣ 가-힣]')  # 정규 표현식 처리\n",
        "    result = hangul.sub('', text)\n",
        "    okt = Okt()  # 형태소 추출\n",
        "    nouns = okt.nouns(result)\n",
        "    nouns = [x for x in nouns if len(x) > 1]  # 한글자 키워드 제거\n",
        "    nouns = [x for x in nouns if x not in stopwords]  # 불용어 제거\n",
        "    return nouns\n",
        "\n",
        "vect = CountVectorizer(tokenizer = lambda x: text_cleaning(x))\n",
        "bow_vect = vect.fit_transform(df['review_text'].tolist())\n",
        "word_list = vect.get_feature_names_out()\n",
        "count_list = bow_vect.toarray().sum(axis=0)\n",
        "# 단어 리스트\n",
        "word_list\n",
        "# 각 단어가 전체 리뷰중에 등장한 총 횟수\n",
        "count_list\n",
        "# 각 단어의 리뷰별 등장 횟수\n",
        "bow_vect.toarray()\n",
        "# \"단어\" - \"총 등장 횟수\" Matching\n",
        "\n",
        "word_count_dict = dict(zip(word_list, count_list))\n",
        "word_count_dict\n",
        "def rating_to_label(rating):\n",
        "    if rating > 3:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "df['y'] = df['rating'].apply(lambda x: rating_to_label(x))\n",
        "\n",
        "#분류 모델 베이스라인\n",
        "\n",
        "# 필요한 라이브러리 import\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 데이터 준비\n",
        "X = df['review_text']  # 텍스트 데이터\n",
        "y = df['y']  # 레이블 (긍정: 1, 부정: 0)\n",
        "\n",
        "# 훈련 데이터와 테스트 데이터로 분리\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 텍스트 데이터 전처리 함수 (앞서 구현한 것과 비슷하게)\n",
        "def text_preprocessing(text):\n",
        "    # 원하는 전처리 작업을 수행\n",
        "    text = apply_regular_expression(text)\n",
        "    nouns = text_cleaning(text)\n",
        "    return ' '.join(nouns)  # 명사들을 공백으로 구분하여 하나의 문자열로 만듦\n",
        "\n",
        "X_train = X_train.apply(text_preprocessing)\n",
        "X_test = X_test.apply(text_preprocessing)\n",
        "\n",
        "# 텍스트 데이터를 벡터로 변환 (위에서 사용한 CountVectorizer 사용)\n",
        "vect = CountVectorizer(tokenizer=lambda x: x)\n",
        "X_train_bow = vect.fit_transform(X_train)\n",
        "X_test_bow = vect.transform(X_test)\n",
        "\n",
        "# 분류 모델 학습\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X_train_bow, y_train)\n",
        "\n",
        "# 예측\n",
        "y_pred = model.predict(X_test_bow)\n",
        "\n",
        "# 정확도 평가\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"모델 정확도: {accuracy}\")\n",
        "\n",
        "# 예측 함수 (텍스트를 입력으로 받아 긍정 또는 부정 예측)\n",
        "def predict_sentiment(text):\n",
        "    preprocessed_text = text_preprocessing(text)\n",
        "    text_vector = vect.transform([preprocessed_text])\n",
        "    prediction = model.predict(text_vector)[0]\n",
        "    return prediction\n",
        "\n",
        "# 예측 테스트\n",
        "sample_text = \"이 수유실은 정말 편리하고 깨끗해요!\"\n",
        "result = predict_sentiment(sample_text)\n",
        "print(f\"텍스트 감정: {result}\")\n",
        "\n",
        "df['predicted_sentiment'] = df['review_text'].apply(predict_sentiment)\n",
        "# 수유실별로 예측된 감정 점수의 평균을 계산합니다.\n",
        "sentiment_scores = df.groupby('facility')['predicted_sentiment'].mean().reset_index()\n",
        "sentiment_scores = sentiment_scores['predicted_sentiment'].tolist()\n",
        "# 결과 출력\n",
        "print(\"sentiment_scores\")\n",
        "print(sentiment_scores, sep = '\\n')\n",
        "#============================================================================================\n",
        "\n",
        "# 5. 감정점수, 평균 평점, 거리를 고려하여 수유실을 추천합니다.\n",
        "# 각 수유실에 대한 종합 점수를 계산합니다.\n",
        "combined_scores = []\n",
        "for i in range(20):\n",
        "    score = sentiment_scores[i] * 0.5 + reviews_ratings[i] * 0.3 - distances[0][i] * 0.2\n",
        "    combined_scores.append((score, i))\n",
        "\n",
        "# 종합 점수가 가장 높은 5개의 수유실을 추천합니다.\n",
        "recommended_indices = sorted(combined_scores, reverse=True)[:5]\n",
        "\n",
        "# 추천된 수유실의 위치를 가져옵니다.\n",
        "recommended_rooms = [(nursing_rooms.iloc[idx][0], nursing_rooms.iloc[idx][1]) for _, idx in recommended_indices]\n",
        "\n",
        "print(\"사용자 추천 리스트\")\n",
        "print(recommended_rooms)\n",
        "\n",
        "\n",
        "# 6. 백엔드로 사용자 ID와 추천된 수유실 위치를 전송합니다.\n",
        "# 실제로는 API 호출을 통해 전송하지만, 여기서는 출력으로 대체합니다.\n",
        "backend_data = {\n",
        "    \"user_id\": user_id,\n",
        "    \"recommended_rooms\": recommended_rooms\n",
        "}\n",
        "\n",
        "backend_data  # 이 데이터를 백엔드로 전송하게 됩니다."
      ]
    }
  ]
}